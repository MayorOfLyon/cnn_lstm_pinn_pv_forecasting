{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f9beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d784c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def preprocess_data(train, test, sequence_length):\n",
    "    train_labels = train['实际功率']\n",
    "    train_features = train.drop(columns=['实际功率'])\n",
    "    test_labels = test['实际功率']\n",
    "    test_features = test.drop(columns=['实际功率'])\n",
    "    \n",
    "    train_labels = np.array(train_labels).reshape(-1, 1)\n",
    "    test_labels = np.array(test_labels).reshape(-1, 1)\n",
    "    \n",
    "    scaler1 = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_features = scaler1.fit_transform(train_features)\n",
    "    test_features = scaler1.transform(test_features)\n",
    "    \n",
    "    scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_labels = scaler2.fit_transform(train_labels)\n",
    "    test_labels = scaler2.transform(test_labels)\n",
    "    \n",
    "    X_train, y_train = [], []\n",
    "    X_test, y_test = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train.append(train_features[i:i + sequence_length])  # 特征\n",
    "        y_train.append(train_labels[i + sequence_length-1])  # 目标值（功率输出）\n",
    "        \n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test.append(test_features[i:i + sequence_length])  # 特征\n",
    "        y_test.append(test_labels[i + sequence_length-1])  # 目标值（功率输出）\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    return X_train, y_train, X_test, y_test, scaler2\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# 定义 CNN-LSTM 模型\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_channels,middle_channels1, middle_channels2, out_channels, hidden_size1, hidden_size2, hidden_size3):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_channels=input_channels, out_channels = middle_channels1)\n",
    "        self.conv2 = ConvBlock(in_channels=middle_channels1, out_channels = middle_channels2)\n",
    "        self.conv3 = ConvBlock(in_channels=middle_channels2, out_channels = out_channels)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=hidden_size1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size1, hidden_size=hidden_size2, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_size2, hidden_size=hidden_size3, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size3, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # 调整维度以适应卷积层的输入要求\n",
    "        # convblocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # maxpooling\n",
    "        x = self.pooling(x)\n",
    "        # # flatten\n",
    "        x = x.permute(0, 2, 1)  # 调整维度以适应 LSTM 的输入要求\n",
    "        # lstm1\n",
    "        h0 = torch.zeros(self.lstm1.num_layers, x.size(0), self.lstm1.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm1.num_layers, x.size(0), self.lstm1.hidden_size).to(x.device)\n",
    "        x, _ = self.lstm1(x, (h0, c0))\n",
    "        x = self.relu(x)\n",
    "        # lstm2\n",
    "        h0 = torch.zeros(self.lstm2.num_layers, x.size(0), self.lstm2.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm2.num_layers, x.size(0), self.lstm2.hidden_size).to(x.device)\n",
    "        x, _ = self.lstm2(x, (h0, c0))\n",
    "        x = self.relu(x)\n",
    "        # lstm3\n",
    "        h0 = torch.zeros(self.lstm3.num_layers, x.size(0), self.lstm3.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm3.num_layers, x.size(0), self.lstm3.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm3(x, (h0, c0))\n",
    "        out = self.relu(out[:, -1, :])\n",
    "        # fc\n",
    "        out = self.fc1(out)  # 取 LSTM 的最后一个时间步的输出\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4e4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "train = pd.read_csv('../dataset/prepared_dataset/train_{i}.csv'.format(i=i))\n",
    "test = pd.read_csv('../dataset/prepared_dataset/test_{i}.csv'.format(i=i))\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671f4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)  # 如果使用GPU\n",
    "torch.cuda.manual_seed_all(seed)  # 如果使用多个GPU\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5146b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 24 * 4 # 假设使用 24 小时的历史数据进行预测\n",
    "lr = 0.001\n",
    "X_train, y_train, X_test, y_test, scaler = preprocess_data(train, test, sequence_length)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Training on {device}')\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "input_channels = X_train.shape[2]  # 输入特征的数量\n",
    "middle_channels1 = 16\n",
    "middle_channels2 = 32\n",
    "out_channels = 64\n",
    "hidden_size1 = 16\n",
    "hidden_size2 = 32\n",
    "hidden_size3 = 64\n",
    "\n",
    "model = CNNLSTMModel(input_channels = input_channels,middle_channels1 = middle_channels1, middle_channels2 = middle_channels2, \n",
    "                     out_channels = out_channels, hidden_size1 = hidden_size1, hidden_size2 = hidden_size2,hidden_size3 = hidden_size3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbf28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [1/358], Loss: 0.0968\n",
      "Epoch [1/50], Step [11/358], Loss: 0.0767\n",
      "Epoch [1/50], Step [21/358], Loss: 0.0868\n",
      "Epoch [1/50], Step [31/358], Loss: 0.0793\n",
      "Epoch [1/50], Step [41/358], Loss: 0.0649\n",
      "Epoch [1/50], Step [51/358], Loss: 0.0706\n",
      "Epoch [1/50], Step [61/358], Loss: 0.0823\n",
      "Epoch [1/50], Step [71/358], Loss: 0.0825\n",
      "Epoch [1/50], Step [81/358], Loss: 0.0482\n",
      "Epoch [1/50], Step [91/358], Loss: 0.0250\n",
      "Epoch [1/50], Step [101/358], Loss: 0.0076\n",
      "Epoch [1/50], Step [111/358], Loss: 0.0099\n",
      "Epoch [1/50], Step [121/358], Loss: 0.0091\n",
      "Epoch [1/50], Step [131/358], Loss: 0.0164\n",
      "Epoch [1/50], Step [141/358], Loss: 0.0045\n",
      "Epoch [1/50], Step [151/358], Loss: 0.0066\n",
      "Epoch [1/50], Step [161/358], Loss: 0.0084\n",
      "Epoch [1/50], Step [171/358], Loss: 0.0111\n",
      "Epoch [1/50], Step [181/358], Loss: 0.0098\n",
      "Epoch [1/50], Step [191/358], Loss: 0.0088\n",
      "Epoch [1/50], Step [201/358], Loss: 0.0129\n",
      "Epoch [1/50], Step [211/358], Loss: 0.0065\n",
      "Epoch [1/50], Step [221/358], Loss: 0.0075\n",
      "Epoch [1/50], Step [231/358], Loss: 0.0049\n",
      "Epoch [1/50], Step [241/358], Loss: 0.0057\n",
      "Epoch [1/50], Step [251/358], Loss: 0.0138\n",
      "Epoch [1/50], Step [261/358], Loss: 0.0065\n",
      "Epoch [1/50], Step [271/358], Loss: 0.0034\n",
      "Epoch [1/50], Step [281/358], Loss: 0.0062\n",
      "Epoch [1/50], Step [291/358], Loss: 0.0022\n",
      "Epoch [1/50], Step [301/358], Loss: 0.0037\n",
      "Epoch [1/50], Step [311/358], Loss: 0.0049\n",
      "Epoch [1/50], Step [321/358], Loss: 0.0090\n",
      "Epoch [1/50], Step [331/358], Loss: 0.0069\n",
      "Epoch [1/50], Step [341/358], Loss: 0.0081\n",
      "Epoch [1/50], Step [351/358], Loss: 0.0026\n",
      "Epoch [2/50], Step [1/358], Loss: 0.0059\n",
      "Epoch [2/50], Step [11/358], Loss: 0.0061\n",
      "Epoch [2/50], Step [21/358], Loss: 0.0101\n",
      "Epoch [2/50], Step [31/358], Loss: 0.0065\n",
      "Epoch [2/50], Step [41/358], Loss: 0.0043\n",
      "Epoch [2/50], Step [51/358], Loss: 0.0067\n",
      "Epoch [2/50], Step [61/358], Loss: 0.0056\n",
      "Epoch [2/50], Step [71/358], Loss: 0.0035\n",
      "Epoch [2/50], Step [81/358], Loss: 0.0034\n",
      "Epoch [2/50], Step [91/358], Loss: 0.0045\n",
      "Epoch [2/50], Step [101/358], Loss: 0.0078\n",
      "Epoch [2/50], Step [111/358], Loss: 0.0057\n",
      "Epoch [2/50], Step [121/358], Loss: 0.0050\n",
      "Epoch [2/50], Step [131/358], Loss: 0.0080\n",
      "Epoch [2/50], Step [141/358], Loss: 0.0035\n",
      "Epoch [2/50], Step [151/358], Loss: 0.0102\n",
      "Epoch [2/50], Step [161/358], Loss: 0.0039\n",
      "Epoch [2/50], Step [171/358], Loss: 0.0019\n",
      "Epoch [2/50], Step [181/358], Loss: 0.0052\n",
      "Epoch [2/50], Step [191/358], Loss: 0.0047\n",
      "Epoch [2/50], Step [201/358], Loss: 0.0068\n",
      "Epoch [2/50], Step [211/358], Loss: 0.0072\n",
      "Epoch [2/50], Step [221/358], Loss: 0.0050\n",
      "Epoch [2/50], Step [231/358], Loss: 0.0080\n",
      "Epoch [2/50], Step [241/358], Loss: 0.0067\n",
      "Epoch [2/50], Step [251/358], Loss: 0.0072\n",
      "Epoch [2/50], Step [261/358], Loss: 0.0054\n",
      "Epoch [2/50], Step [271/358], Loss: 0.0019\n",
      "Epoch [2/50], Step [281/358], Loss: 0.0014\n",
      "Epoch [2/50], Step [291/358], Loss: 0.0028\n",
      "Epoch [2/50], Step [301/358], Loss: 0.0070\n",
      "Epoch [2/50], Step [311/358], Loss: 0.0029\n",
      "Epoch [2/50], Step [321/358], Loss: 0.0037\n",
      "Epoch [2/50], Step [331/358], Loss: 0.0010\n",
      "Epoch [2/50], Step [341/358], Loss: 0.0084\n",
      "Epoch [2/50], Step [351/358], Loss: 0.0032\n",
      "Epoch [3/50], Step [1/358], Loss: 0.0040\n",
      "Epoch [3/50], Step [11/358], Loss: 0.0084\n",
      "Epoch [3/50], Step [21/358], Loss: 0.0044\n",
      "Epoch [3/50], Step [31/358], Loss: 0.0065\n",
      "Epoch [3/50], Step [41/358], Loss: 0.0014\n",
      "Epoch [3/50], Step [51/358], Loss: 0.0053\n",
      "Epoch [3/50], Step [61/358], Loss: 0.0081\n",
      "Epoch [3/50], Step [71/358], Loss: 0.0061\n",
      "Epoch [3/50], Step [81/358], Loss: 0.0052\n",
      "Epoch [3/50], Step [91/358], Loss: 0.0056\n",
      "Epoch [3/50], Step [101/358], Loss: 0.0067\n",
      "Epoch [3/50], Step [111/358], Loss: 0.0078\n",
      "Epoch [3/50], Step [121/358], Loss: 0.0067\n",
      "Epoch [3/50], Step [131/358], Loss: 0.0022\n",
      "Epoch [3/50], Step [141/358], Loss: 0.0036\n",
      "Epoch [3/50], Step [151/358], Loss: 0.0070\n",
      "Epoch [3/50], Step [161/358], Loss: 0.0042\n",
      "Epoch [3/50], Step [171/358], Loss: 0.0047\n",
      "Epoch [3/50], Step [181/358], Loss: 0.0045\n",
      "Epoch [3/50], Step [191/358], Loss: 0.0035\n",
      "Epoch [3/50], Step [201/358], Loss: 0.0045\n",
      "Epoch [3/50], Step [211/358], Loss: 0.0088\n",
      "Epoch [3/50], Step [221/358], Loss: 0.0076\n",
      "Epoch [3/50], Step [231/358], Loss: 0.0038\n",
      "Epoch [3/50], Step [241/358], Loss: 0.0013\n",
      "Epoch [3/50], Step [251/358], Loss: 0.0067\n",
      "Epoch [3/50], Step [261/358], Loss: 0.0032\n",
      "Epoch [3/50], Step [271/358], Loss: 0.0030\n",
      "Epoch [3/50], Step [281/358], Loss: 0.0016\n",
      "Epoch [3/50], Step [291/358], Loss: 0.0019\n",
      "Epoch [3/50], Step [301/358], Loss: 0.0042\n",
      "Epoch [3/50], Step [311/358], Loss: 0.0060\n",
      "Epoch [3/50], Step [321/358], Loss: 0.0039\n",
      "Epoch [3/50], Step [331/358], Loss: 0.0043\n",
      "Epoch [3/50], Step [341/358], Loss: 0.0014\n",
      "Epoch [3/50], Step [351/358], Loss: 0.0043\n",
      "Epoch [4/50], Step [1/358], Loss: 0.0016\n",
      "Epoch [4/50], Step [11/358], Loss: 0.0021\n",
      "Epoch [4/50], Step [21/358], Loss: 0.0068\n",
      "Epoch [4/50], Step [31/358], Loss: 0.0025\n",
      "Epoch [4/50], Step [41/358], Loss: 0.0024\n",
      "Epoch [4/50], Step [51/358], Loss: 0.0011\n",
      "Epoch [4/50], Step [61/358], Loss: 0.0016\n",
      "Epoch [4/50], Step [71/358], Loss: 0.0038\n",
      "Epoch [4/50], Step [81/358], Loss: 0.0054\n",
      "Epoch [4/50], Step [91/358], Loss: 0.0020\n",
      "Epoch [4/50], Step [101/358], Loss: 0.0025\n",
      "Epoch [4/50], Step [111/358], Loss: 0.0028\n",
      "Epoch [4/50], Step [121/358], Loss: 0.0027\n",
      "Epoch [4/50], Step [131/358], Loss: 0.0030\n",
      "Epoch [4/50], Step [141/358], Loss: 0.0019\n",
      "Epoch [4/50], Step [151/358], Loss: 0.0015\n",
      "Epoch [4/50], Step [161/358], Loss: 0.0099\n",
      "Epoch [4/50], Step [171/358], Loss: 0.0035\n",
      "Epoch [4/50], Step [181/358], Loss: 0.0014\n",
      "Epoch [4/50], Step [191/358], Loss: 0.0013\n",
      "Epoch [4/50], Step [201/358], Loss: 0.0022\n",
      "Epoch [4/50], Step [211/358], Loss: 0.0035\n",
      "Epoch [4/50], Step [221/358], Loss: 0.0025\n",
      "Epoch [4/50], Step [231/358], Loss: 0.0032\n",
      "Epoch [4/50], Step [241/358], Loss: 0.0046\n",
      "Epoch [4/50], Step [251/358], Loss: 0.0111\n",
      "Epoch [4/50], Step [261/358], Loss: 0.0053\n",
      "Epoch [4/50], Step [271/358], Loss: 0.0019\n",
      "Epoch [4/50], Step [281/358], Loss: 0.0023\n",
      "Epoch [4/50], Step [291/358], Loss: 0.0026\n",
      "Epoch [4/50], Step [301/358], Loss: 0.0044\n",
      "Epoch [4/50], Step [311/358], Loss: 0.0059\n",
      "Epoch [4/50], Step [321/358], Loss: 0.0052\n",
      "Epoch [4/50], Step [331/358], Loss: 0.0057\n",
      "Epoch [4/50], Step [341/358], Loss: 0.0058\n",
      "Epoch [4/50], Step [351/358], Loss: 0.0032\n",
      "Epoch [5/50], Step [1/358], Loss: 0.0015\n",
      "Epoch [5/50], Step [11/358], Loss: 0.0032\n",
      "Epoch [5/50], Step [21/358], Loss: 0.0018\n",
      "Epoch [5/50], Step [31/358], Loss: 0.0017\n",
      "Epoch [5/50], Step [41/358], Loss: 0.0042\n",
      "Epoch [5/50], Step [51/358], Loss: 0.0036\n",
      "Epoch [5/50], Step [61/358], Loss: 0.0037\n",
      "Epoch [5/50], Step [71/358], Loss: 0.0076\n",
      "Epoch [5/50], Step [81/358], Loss: 0.0008\n",
      "Epoch [5/50], Step [91/358], Loss: 0.0025\n",
      "Epoch [5/50], Step [101/358], Loss: 0.0024\n",
      "Epoch [5/50], Step [111/358], Loss: 0.0020\n",
      "Epoch [5/50], Step [121/358], Loss: 0.0031\n",
      "Epoch [5/50], Step [131/358], Loss: 0.0037\n",
      "Epoch [5/50], Step [141/358], Loss: 0.0061\n",
      "Epoch [5/50], Step [151/358], Loss: 0.0050\n",
      "Epoch [5/50], Step [161/358], Loss: 0.0032\n",
      "Epoch [5/50], Step [171/358], Loss: 0.0070\n",
      "Epoch [5/50], Step [181/358], Loss: 0.0044\n",
      "Epoch [5/50], Step [191/358], Loss: 0.0024\n",
      "Epoch [5/50], Step [201/358], Loss: 0.0019\n",
      "Epoch [5/50], Step [211/358], Loss: 0.0030\n",
      "Epoch [5/50], Step [221/358], Loss: 0.0093\n",
      "Epoch [5/50], Step [231/358], Loss: 0.0021\n",
      "Epoch [5/50], Step [241/358], Loss: 0.0005\n",
      "Epoch [5/50], Step [251/358], Loss: 0.0043\n",
      "Epoch [5/50], Step [261/358], Loss: 0.0062\n",
      "Epoch [5/50], Step [271/358], Loss: 0.0048\n",
      "Epoch [5/50], Step [281/358], Loss: 0.0018\n",
      "Epoch [5/50], Step [291/358], Loss: 0.0020\n",
      "Epoch [5/50], Step [301/358], Loss: 0.0055\n",
      "Epoch [5/50], Step [311/358], Loss: 0.0026\n",
      "Epoch [5/50], Step [321/358], Loss: 0.0015\n",
      "Epoch [5/50], Step [331/358], Loss: 0.0011\n",
      "Epoch [5/50], Step [341/358], Loss: 0.0067\n",
      "Epoch [5/50], Step [351/358], Loss: 0.0022\n",
      "Epoch [6/50], Step [1/358], Loss: 0.0050\n",
      "Epoch [6/50], Step [11/358], Loss: 0.0014\n",
      "Epoch [6/50], Step [21/358], Loss: 0.0012\n",
      "Epoch [6/50], Step [31/358], Loss: 0.0063\n",
      "Epoch [6/50], Step [41/358], Loss: 0.0033\n",
      "Epoch [6/50], Step [51/358], Loss: 0.0043\n",
      "Epoch [6/50], Step [61/358], Loss: 0.0031\n",
      "Epoch [6/50], Step [71/358], Loss: 0.0040\n",
      "Epoch [6/50], Step [81/358], Loss: 0.0043\n",
      "Epoch [6/50], Step [91/358], Loss: 0.0017\n",
      "Epoch [6/50], Step [101/358], Loss: 0.0024\n",
      "Epoch [6/50], Step [111/358], Loss: 0.0026\n",
      "Epoch [6/50], Step [121/358], Loss: 0.0010\n",
      "Epoch [6/50], Step [131/358], Loss: 0.0025\n",
      "Epoch [6/50], Step [141/358], Loss: 0.0032\n",
      "Epoch [6/50], Step [151/358], Loss: 0.0032\n",
      "Epoch [6/50], Step [161/358], Loss: 0.0016\n",
      "Epoch [6/50], Step [171/358], Loss: 0.0027\n",
      "Epoch [6/50], Step [181/358], Loss: 0.0022\n",
      "Epoch [6/50], Step [191/358], Loss: 0.0022\n",
      "Epoch [6/50], Step [201/358], Loss: 0.0014\n",
      "Epoch [6/50], Step [211/358], Loss: 0.0023\n",
      "Epoch [6/50], Step [221/358], Loss: 0.0019\n",
      "Epoch [6/50], Step [231/358], Loss: 0.0028\n",
      "Epoch [6/50], Step [241/358], Loss: 0.0041\n",
      "Epoch [6/50], Step [251/358], Loss: 0.0108\n",
      "Epoch [6/50], Step [261/358], Loss: 0.0030\n",
      "Epoch [6/50], Step [271/358], Loss: 0.0019\n",
      "Epoch [6/50], Step [281/358], Loss: 0.0009\n",
      "Epoch [6/50], Step [291/358], Loss: 0.0029\n",
      "Epoch [6/50], Step [301/358], Loss: 0.0025\n",
      "Epoch [6/50], Step [311/358], Loss: 0.0053\n",
      "Epoch [6/50], Step [321/358], Loss: 0.0006\n",
      "Epoch [6/50], Step [331/358], Loss: 0.0047\n",
      "Epoch [6/50], Step [341/358], Loss: 0.0026\n",
      "Epoch [6/50], Step [351/358], Loss: 0.0023\n",
      "Epoch [7/50], Step [1/358], Loss: 0.0026\n",
      "Epoch [7/50], Step [11/358], Loss: 0.0047\n",
      "Epoch [7/50], Step [21/358], Loss: 0.0002\n",
      "Epoch [7/50], Step [31/358], Loss: 0.0023\n",
      "Epoch [7/50], Step [41/358], Loss: 0.0050\n",
      "Epoch [7/50], Step [51/358], Loss: 0.0018\n",
      "Epoch [7/50], Step [61/358], Loss: 0.0018\n",
      "Epoch [7/50], Step [71/358], Loss: 0.0019\n",
      "Epoch [7/50], Step [81/358], Loss: 0.0029\n",
      "Epoch [7/50], Step [91/358], Loss: 0.0045\n",
      "Epoch [7/50], Step [101/358], Loss: 0.0034\n",
      "Epoch [7/50], Step [111/358], Loss: 0.0012\n",
      "Epoch [7/50], Step [121/358], Loss: 0.0033\n",
      "Epoch [7/50], Step [131/358], Loss: 0.0032\n",
      "Epoch [7/50], Step [141/358], Loss: 0.0023\n",
      "Epoch [7/50], Step [151/358], Loss: 0.0013\n",
      "Epoch [7/50], Step [161/358], Loss: 0.0122\n",
      "Epoch [7/50], Step [171/358], Loss: 0.0020\n",
      "Epoch [7/50], Step [181/358], Loss: 0.0014\n",
      "Epoch [7/50], Step [191/358], Loss: 0.0073\n",
      "Epoch [7/50], Step [201/358], Loss: 0.0064\n",
      "Epoch [7/50], Step [211/358], Loss: 0.0018\n",
      "Epoch [7/50], Step [221/358], Loss: 0.0014\n",
      "Epoch [7/50], Step [231/358], Loss: 0.0008\n",
      "Epoch [7/50], Step [241/358], Loss: 0.0018\n",
      "Epoch [7/50], Step [251/358], Loss: 0.0013\n",
      "Epoch [7/50], Step [261/358], Loss: 0.0010\n",
      "Epoch [7/50], Step [271/358], Loss: 0.0034\n",
      "Epoch [7/50], Step [281/358], Loss: 0.0014\n",
      "Epoch [7/50], Step [291/358], Loss: 0.0029\n",
      "Epoch [7/50], Step [301/358], Loss: 0.0045\n",
      "Epoch [7/50], Step [311/358], Loss: 0.0044\n",
      "Epoch [7/50], Step [321/358], Loss: 0.0017\n",
      "Epoch [7/50], Step [331/358], Loss: 0.0030\n",
      "Epoch [7/50], Step [341/358], Loss: 0.0021\n",
      "Epoch [7/50], Step [351/358], Loss: 0.0042\n",
      "Epoch [8/50], Step [1/358], Loss: 0.0014\n",
      "Epoch [8/50], Step [11/358], Loss: 0.0026\n",
      "Epoch [8/50], Step [21/358], Loss: 0.0010\n",
      "Epoch [8/50], Step [31/358], Loss: 0.0008\n",
      "Epoch [8/50], Step [41/358], Loss: 0.0023\n",
      "Epoch [8/50], Step [51/358], Loss: 0.0014\n",
      "Epoch [8/50], Step [61/358], Loss: 0.0015\n",
      "Epoch [8/50], Step [71/358], Loss: 0.0074\n",
      "Epoch [8/50], Step [81/358], Loss: 0.0028\n",
      "Epoch [8/50], Step [91/358], Loss: 0.0012\n",
      "Epoch [8/50], Step [101/358], Loss: 0.0035\n",
      "Epoch [8/50], Step [111/358], Loss: 0.0013\n",
      "Epoch [8/50], Step [121/358], Loss: 0.0017\n",
      "Epoch [8/50], Step [131/358], Loss: 0.0047\n",
      "Epoch [8/50], Step [141/358], Loss: 0.0011\n",
      "Epoch [8/50], Step [151/358], Loss: 0.0031\n",
      "Epoch [8/50], Step [161/358], Loss: 0.0028\n",
      "Epoch [8/50], Step [171/358], Loss: 0.0013\n",
      "Epoch [8/50], Step [181/358], Loss: 0.0019\n",
      "Epoch [8/50], Step [191/358], Loss: 0.0066\n",
      "Epoch [8/50], Step [201/358], Loss: 0.0011\n",
      "Epoch [8/50], Step [211/358], Loss: 0.0013\n",
      "Epoch [8/50], Step [221/358], Loss: 0.0021\n",
      "Epoch [8/50], Step [231/358], Loss: 0.0009\n",
      "Epoch [8/50], Step [241/358], Loss: 0.0065\n",
      "Epoch [8/50], Step [251/358], Loss: 0.0025\n",
      "Epoch [8/50], Step [261/358], Loss: 0.0040\n",
      "Epoch [8/50], Step [271/358], Loss: 0.0011\n",
      "Epoch [8/50], Step [281/358], Loss: 0.0054\n",
      "Epoch [8/50], Step [291/358], Loss: 0.0039\n",
      "Epoch [8/50], Step [301/358], Loss: 0.0010\n",
      "Epoch [8/50], Step [311/358], Loss: 0.0023\n",
      "Epoch [8/50], Step [321/358], Loss: 0.0037\n",
      "Epoch [8/50], Step [331/358], Loss: 0.0015\n",
      "Epoch [8/50], Step [341/358], Loss: 0.0052\n",
      "Epoch [8/50], Step [351/358], Loss: 0.0035\n",
      "Epoch [9/50], Step [1/358], Loss: 0.0006\n",
      "Epoch [9/50], Step [11/358], Loss: 0.0018\n",
      "Epoch [9/50], Step [21/358], Loss: 0.0011\n",
      "Epoch [9/50], Step [31/358], Loss: 0.0010\n",
      "Epoch [9/50], Step [41/358], Loss: 0.0009\n",
      "Epoch [9/50], Step [51/358], Loss: 0.0009\n",
      "Epoch [9/50], Step [61/358], Loss: 0.0009\n",
      "Epoch [9/50], Step [71/358], Loss: 0.0065\n",
      "Epoch [9/50], Step [81/358], Loss: 0.0058\n",
      "Epoch [9/50], Step [91/358], Loss: 0.0007\n",
      "Epoch [9/50], Step [101/358], Loss: 0.0046\n",
      "Epoch [9/50], Step [111/358], Loss: 0.0016\n",
      "Epoch [9/50], Step [121/358], Loss: 0.0007\n",
      "Epoch [9/50], Step [131/358], Loss: 0.0011\n",
      "Epoch [9/50], Step [141/358], Loss: 0.0017\n",
      "Epoch [9/50], Step [151/358], Loss: 0.0012\n",
      "Epoch [9/50], Step [161/358], Loss: 0.0032\n",
      "Epoch [9/50], Step [171/358], Loss: 0.0025\n",
      "Epoch [9/50], Step [181/358], Loss: 0.0013\n",
      "Epoch [9/50], Step [191/358], Loss: 0.0014\n",
      "Epoch [9/50], Step [201/358], Loss: 0.0038\n",
      "Epoch [9/50], Step [211/358], Loss: 0.0062\n",
      "Epoch [9/50], Step [221/358], Loss: 0.0022\n",
      "Epoch [9/50], Step [231/358], Loss: 0.0014\n",
      "Epoch [9/50], Step [241/358], Loss: 0.0014\n",
      "Epoch [9/50], Step [251/358], Loss: 0.0007\n",
      "Epoch [9/50], Step [261/358], Loss: 0.0029\n",
      "Epoch [9/50], Step [271/358], Loss: 0.0047\n",
      "Epoch [9/50], Step [281/358], Loss: 0.0013\n",
      "Epoch [9/50], Step [291/358], Loss: 0.0031\n",
      "Epoch [9/50], Step [301/358], Loss: 0.0011\n",
      "Epoch [9/50], Step [311/358], Loss: 0.0015\n",
      "Epoch [9/50], Step [321/358], Loss: 0.0015\n",
      "Epoch [9/50], Step [331/358], Loss: 0.0045\n",
      "Epoch [9/50], Step [341/358], Loss: 0.0006\n",
      "Epoch [9/50], Step [351/358], Loss: 0.0014\n",
      "Epoch [10/50], Step [1/358], Loss: 0.0012\n",
      "Epoch [10/50], Step [11/358], Loss: 0.0014\n",
      "Epoch [10/50], Step [21/358], Loss: 0.0013\n",
      "Epoch [10/50], Step [31/358], Loss: 0.0024\n",
      "Epoch [10/50], Step [41/358], Loss: 0.0027\n",
      "Epoch [10/50], Step [51/358], Loss: 0.0018\n",
      "Epoch [10/50], Step [61/358], Loss: 0.0020\n",
      "Epoch [10/50], Step [71/358], Loss: 0.0020\n",
      "Epoch [10/50], Step [81/358], Loss: 0.0015\n",
      "Epoch [10/50], Step [91/358], Loss: 0.0006\n",
      "Epoch [10/50], Step [101/358], Loss: 0.0006\n",
      "Epoch [10/50], Step [111/358], Loss: 0.0012\n",
      "Epoch [10/50], Step [121/358], Loss: 0.0018\n",
      "Epoch [10/50], Step [131/358], Loss: 0.0015\n",
      "Epoch [10/50], Step [141/358], Loss: 0.0049\n",
      "Epoch [10/50], Step [151/358], Loss: 0.0009\n",
      "Epoch [10/50], Step [161/358], Loss: 0.0012\n",
      "Epoch [10/50], Step [171/358], Loss: 0.0039\n",
      "Epoch [10/50], Step [181/358], Loss: 0.0010\n",
      "Epoch [10/50], Step [191/358], Loss: 0.0014\n",
      "Epoch [10/50], Step [201/358], Loss: 0.0031\n",
      "Epoch [10/50], Step [211/358], Loss: 0.0026\n",
      "Epoch [10/50], Step [221/358], Loss: 0.0027\n",
      "Epoch [10/50], Step [231/358], Loss: 0.0018\n",
      "Epoch [10/50], Step [241/358], Loss: 0.0009\n",
      "Epoch [10/50], Step [251/358], Loss: 0.0027\n",
      "Epoch [10/50], Step [261/358], Loss: 0.0023\n",
      "Epoch [10/50], Step [271/358], Loss: 0.0024\n",
      "Epoch [10/50], Step [281/358], Loss: 0.0048\n",
      "Epoch [10/50], Step [291/358], Loss: 0.0018\n",
      "Epoch [10/50], Step [301/358], Loss: 0.0026\n",
      "Epoch [10/50], Step [311/358], Loss: 0.0027\n",
      "Epoch [10/50], Step [321/358], Loss: 0.0040\n",
      "Epoch [10/50], Step [331/358], Loss: 0.0030\n",
      "Epoch [10/50], Step [341/358], Loss: 0.0023\n",
      "Epoch [10/50], Step [351/358], Loss: 0.0024\n",
      "Epoch [11/50], Step [1/358], Loss: 0.0016\n",
      "Epoch [11/50], Step [11/358], Loss: 0.0033\n",
      "Epoch [11/50], Step [21/358], Loss: 0.0059\n",
      "Epoch [11/50], Step [31/358], Loss: 0.0029\n",
      "Epoch [11/50], Step [41/358], Loss: 0.0033\n",
      "Epoch [11/50], Step [51/358], Loss: 0.0006\n",
      "Epoch [11/50], Step [61/358], Loss: 0.0010\n",
      "Epoch [11/50], Step [71/358], Loss: 0.0063\n",
      "Epoch [11/50], Step [81/358], Loss: 0.0017\n",
      "Epoch [11/50], Step [91/358], Loss: 0.0018\n",
      "Epoch [11/50], Step [101/358], Loss: 0.0021\n",
      "Epoch [11/50], Step [111/358], Loss: 0.0024\n",
      "Epoch [11/50], Step [121/358], Loss: 0.0026\n",
      "Epoch [11/50], Step [131/358], Loss: 0.0011\n",
      "Epoch [11/50], Step [141/358], Loss: 0.0025\n",
      "Epoch [11/50], Step [151/358], Loss: 0.0009\n",
      "Epoch [11/50], Step [161/358], Loss: 0.0015\n",
      "Epoch [11/50], Step [171/358], Loss: 0.0006\n",
      "Epoch [11/50], Step [181/358], Loss: 0.0023\n",
      "Epoch [11/50], Step [191/358], Loss: 0.0015\n",
      "Epoch [11/50], Step [201/358], Loss: 0.0014\n",
      "Epoch [11/50], Step [211/358], Loss: 0.0012\n",
      "Epoch [11/50], Step [221/358], Loss: 0.0008\n",
      "Epoch [11/50], Step [231/358], Loss: 0.0010\n",
      "Epoch [11/50], Step [241/358], Loss: 0.0008\n",
      "Epoch [11/50], Step [251/358], Loss: 0.0014\n",
      "Epoch [11/50], Step [261/358], Loss: 0.0085\n",
      "Epoch [11/50], Step [271/358], Loss: 0.0009\n",
      "Epoch [11/50], Step [281/358], Loss: 0.0010\n",
      "Epoch [11/50], Step [291/358], Loss: 0.0007\n",
      "Epoch [11/50], Step [301/358], Loss: 0.0011\n",
      "Epoch [11/50], Step [311/358], Loss: 0.0008\n",
      "Epoch [11/50], Step [321/358], Loss: 0.0012\n",
      "Epoch [11/50], Step [331/358], Loss: 0.0011\n",
      "Epoch [11/50], Step [341/358], Loss: 0.0022\n",
      "Epoch [11/50], Step [351/358], Loss: 0.0041\n",
      "Epoch [12/50], Step [1/358], Loss: 0.0013\n",
      "Epoch [12/50], Step [11/358], Loss: 0.0036\n",
      "Epoch [12/50], Step [21/358], Loss: 0.0044\n",
      "Epoch [12/50], Step [31/358], Loss: 0.0013\n",
      "Epoch [12/50], Step [41/358], Loss: 0.0009\n",
      "Epoch [12/50], Step [51/358], Loss: 0.0034\n",
      "Epoch [12/50], Step [61/358], Loss: 0.0014\n",
      "Epoch [12/50], Step [71/358], Loss: 0.0024\n",
      "Epoch [12/50], Step [81/358], Loss: 0.0012\n",
      "Epoch [12/50], Step [91/358], Loss: 0.0022\n",
      "Epoch [12/50], Step [101/358], Loss: 0.0010\n",
      "Epoch [12/50], Step [111/358], Loss: 0.0014\n",
      "Epoch [12/50], Step [121/358], Loss: 0.0062\n",
      "Epoch [12/50], Step [131/358], Loss: 0.0020\n",
      "Epoch [12/50], Step [141/358], Loss: 0.0011\n",
      "Epoch [12/50], Step [151/358], Loss: 0.0027\n",
      "Epoch [12/50], Step [161/358], Loss: 0.0029\n",
      "Epoch [12/50], Step [171/358], Loss: 0.0036\n",
      "Epoch [12/50], Step [181/358], Loss: 0.0007\n",
      "Epoch [12/50], Step [191/358], Loss: 0.0009\n",
      "Epoch [12/50], Step [201/358], Loss: 0.0009\n",
      "Epoch [12/50], Step [211/358], Loss: 0.0020\n",
      "Epoch [12/50], Step [221/358], Loss: 0.0009\n",
      "Epoch [12/50], Step [231/358], Loss: 0.0015\n",
      "Epoch [12/50], Step [241/358], Loss: 0.0009\n",
      "Epoch [12/50], Step [251/358], Loss: 0.0023\n",
      "Epoch [12/50], Step [261/358], Loss: 0.0008\n",
      "Epoch [12/50], Step [271/358], Loss: 0.0020\n",
      "Epoch [12/50], Step [281/358], Loss: 0.0014\n",
      "Epoch [12/50], Step [291/358], Loss: 0.0066\n",
      "Epoch [12/50], Step [301/358], Loss: 0.0024\n",
      "Epoch [12/50], Step [311/358], Loss: 0.0010\n",
      "Epoch [12/50], Step [321/358], Loss: 0.0026\n",
      "Epoch [12/50], Step [331/358], Loss: 0.0047\n",
      "Epoch [12/50], Step [341/358], Loss: 0.0009\n",
      "Epoch [12/50], Step [351/358], Loss: 0.0018\n",
      "Epoch [13/50], Step [1/358], Loss: 0.0046\n",
      "Epoch [13/50], Step [11/358], Loss: 0.0010\n",
      "Epoch [13/50], Step [21/358], Loss: 0.0022\n",
      "Epoch [13/50], Step [31/358], Loss: 0.0045\n",
      "Epoch [13/50], Step [41/358], Loss: 0.0014\n",
      "Epoch [13/50], Step [51/358], Loss: 0.0019\n",
      "Epoch [13/50], Step [61/358], Loss: 0.0014\n",
      "Epoch [13/50], Step [71/358], Loss: 0.0010\n",
      "Epoch [13/50], Step [81/358], Loss: 0.0019\n",
      "Epoch [13/50], Step [91/358], Loss: 0.0013\n",
      "Epoch [13/50], Step [101/358], Loss: 0.0014\n",
      "Epoch [13/50], Step [111/358], Loss: 0.0021\n",
      "Epoch [13/50], Step [121/358], Loss: 0.0040\n",
      "Epoch [13/50], Step [131/358], Loss: 0.0018\n",
      "Epoch [13/50], Step [141/358], Loss: 0.0011\n",
      "Epoch [13/50], Step [151/358], Loss: 0.0007\n",
      "Epoch [13/50], Step [161/358], Loss: 0.0022\n",
      "Epoch [13/50], Step [171/358], Loss: 0.0034\n",
      "Epoch [13/50], Step [181/358], Loss: 0.0007\n",
      "Epoch [13/50], Step [191/358], Loss: 0.0010\n",
      "Epoch [13/50], Step [201/358], Loss: 0.0086\n",
      "Epoch [13/50], Step [211/358], Loss: 0.0014\n",
      "Epoch [13/50], Step [221/358], Loss: 0.0013\n",
      "Epoch [13/50], Step [231/358], Loss: 0.0008\n",
      "Epoch [13/50], Step [241/358], Loss: 0.0011\n",
      "Epoch [13/50], Step [251/358], Loss: 0.0021\n",
      "Epoch [13/50], Step [261/358], Loss: 0.0014\n",
      "Epoch [13/50], Step [271/358], Loss: 0.0045\n",
      "Epoch [13/50], Step [281/358], Loss: 0.0010\n",
      "Epoch [13/50], Step [291/358], Loss: 0.0040\n",
      "Epoch [13/50], Step [301/358], Loss: 0.0010\n",
      "Epoch [13/50], Step [311/358], Loss: 0.0014\n",
      "Epoch [13/50], Step [321/358], Loss: 0.0031\n",
      "Epoch [13/50], Step [331/358], Loss: 0.0020\n",
      "Epoch [13/50], Step [341/358], Loss: 0.0010\n",
      "Epoch [13/50], Step [351/358], Loss: 0.0017\n",
      "Epoch [14/50], Step [1/358], Loss: 0.0006\n",
      "Epoch [14/50], Step [11/358], Loss: 0.0036\n",
      "Epoch [14/50], Step [21/358], Loss: 0.0015\n",
      "Epoch [14/50], Step [31/358], Loss: 0.0008\n",
      "Epoch [14/50], Step [41/358], Loss: 0.0028\n",
      "Epoch [14/50], Step [51/358], Loss: 0.0028\n",
      "Epoch [14/50], Step [61/358], Loss: 0.0031\n",
      "Epoch [14/50], Step [71/358], Loss: 0.0012\n",
      "Epoch [14/50], Step [81/358], Loss: 0.0011\n",
      "Epoch [14/50], Step [91/358], Loss: 0.0017\n",
      "Epoch [14/50], Step [101/358], Loss: 0.0030\n",
      "Epoch [14/50], Step [111/358], Loss: 0.0015\n",
      "Epoch [14/50], Step [121/358], Loss: 0.0008\n",
      "Epoch [14/50], Step [131/358], Loss: 0.0032\n",
      "Epoch [14/50], Step [141/358], Loss: 0.0012\n",
      "Epoch [14/50], Step [151/358], Loss: 0.0025\n",
      "Epoch [14/50], Step [161/358], Loss: 0.0006\n",
      "Epoch [14/50], Step [171/358], Loss: 0.0011\n",
      "Epoch [14/50], Step [181/358], Loss: 0.0063\n",
      "Epoch [14/50], Step [191/358], Loss: 0.0006\n",
      "Epoch [14/50], Step [201/358], Loss: 0.0054\n",
      "Epoch [14/50], Step [211/358], Loss: 0.0036\n",
      "Epoch [14/50], Step [221/358], Loss: 0.0005\n",
      "Epoch [14/50], Step [231/358], Loss: 0.0039\n",
      "Epoch [14/50], Step [241/358], Loss: 0.0007\n",
      "Epoch [14/50], Step [251/358], Loss: 0.0007\n",
      "Epoch [14/50], Step [261/358], Loss: 0.0012\n",
      "Epoch [14/50], Step [271/358], Loss: 0.0010\n",
      "Epoch [14/50], Step [281/358], Loss: 0.0024\n",
      "Epoch [14/50], Step [291/358], Loss: 0.0016\n",
      "Epoch [14/50], Step [301/358], Loss: 0.0017\n",
      "Epoch [14/50], Step [311/358], Loss: 0.0029\n",
      "Epoch [14/50], Step [321/358], Loss: 0.0022\n",
      "Epoch [14/50], Step [331/358], Loss: 0.0017\n",
      "Epoch [14/50], Step [341/358], Loss: 0.0014\n",
      "Epoch [14/50], Step [351/358], Loss: 0.0009\n",
      "Epoch [15/50], Step [1/358], Loss: 0.0024\n",
      "Epoch [15/50], Step [11/358], Loss: 0.0023\n",
      "Epoch [15/50], Step [21/358], Loss: 0.0017\n",
      "Epoch [15/50], Step [31/358], Loss: 0.0008\n",
      "Epoch [15/50], Step [41/358], Loss: 0.0010\n",
      "Epoch [15/50], Step [51/358], Loss: 0.0017\n",
      "Epoch [15/50], Step [61/358], Loss: 0.0012\n",
      "Epoch [15/50], Step [71/358], Loss: 0.0022\n",
      "Epoch [15/50], Step [81/358], Loss: 0.0017\n",
      "Epoch [15/50], Step [91/358], Loss: 0.0007\n",
      "Epoch [15/50], Step [101/358], Loss: 0.0010\n",
      "Epoch [15/50], Step [111/358], Loss: 0.0061\n",
      "Epoch [15/50], Step [121/358], Loss: 0.0021\n",
      "Epoch [15/50], Step [131/358], Loss: 0.0010\n",
      "Epoch [15/50], Step [141/358], Loss: 0.0024\n",
      "Epoch [15/50], Step [151/358], Loss: 0.0029\n",
      "Epoch [15/50], Step [161/358], Loss: 0.0037\n",
      "Epoch [15/50], Step [171/358], Loss: 0.0016\n",
      "Epoch [15/50], Step [181/358], Loss: 0.0011\n",
      "Epoch [15/50], Step [191/358], Loss: 0.0006\n",
      "Epoch [15/50], Step [201/358], Loss: 0.0047\n",
      "Epoch [15/50], Step [211/358], Loss: 0.0007\n",
      "Epoch [15/50], Step [221/358], Loss: 0.0028\n",
      "Epoch [15/50], Step [231/358], Loss: 0.0011\n",
      "Epoch [15/50], Step [241/358], Loss: 0.0012\n",
      "Epoch [15/50], Step [251/358], Loss: 0.0008\n",
      "Epoch [15/50], Step [261/358], Loss: 0.0017\n",
      "Epoch [15/50], Step [271/358], Loss: 0.0036\n",
      "Epoch [15/50], Step [281/358], Loss: 0.0013\n",
      "Epoch [15/50], Step [291/358], Loss: 0.0006\n",
      "Epoch [15/50], Step [301/358], Loss: 0.0032\n",
      "Epoch [15/50], Step [311/358], Loss: 0.0005\n",
      "Epoch [15/50], Step [321/358], Loss: 0.0020\n",
      "Epoch [15/50], Step [331/358], Loss: 0.0020\n",
      "Epoch [15/50], Step [341/358], Loss: 0.0017\n",
      "Epoch [15/50], Step [351/358], Loss: 0.0013\n",
      "Epoch [16/50], Step [1/358], Loss: 0.0008\n",
      "Epoch [16/50], Step [11/358], Loss: 0.0022\n",
      "Epoch [16/50], Step [21/358], Loss: 0.0060\n",
      "Epoch [16/50], Step [31/358], Loss: 0.0023\n",
      "Epoch [16/50], Step [41/358], Loss: 0.0012\n",
      "Epoch [16/50], Step [51/358], Loss: 0.0014\n",
      "Epoch [16/50], Step [61/358], Loss: 0.0039\n",
      "Epoch [16/50], Step [71/358], Loss: 0.0029\n",
      "Epoch [16/50], Step [81/358], Loss: 0.0032\n",
      "Epoch [16/50], Step [91/358], Loss: 0.0009\n",
      "Epoch [16/50], Step [101/358], Loss: 0.0078\n",
      "Epoch [16/50], Step [111/358], Loss: 0.0020\n",
      "Epoch [16/50], Step [121/358], Loss: 0.0022\n",
      "Epoch [16/50], Step [131/358], Loss: 0.0010\n",
      "Epoch [16/50], Step [141/358], Loss: 0.0017\n",
      "Epoch [16/50], Step [151/358], Loss: 0.0012\n",
      "Epoch [16/50], Step [161/358], Loss: 0.0006\n",
      "Epoch [16/50], Step [171/358], Loss: 0.0013\n",
      "Epoch [16/50], Step [181/358], Loss: 0.0012\n",
      "Epoch [16/50], Step [191/358], Loss: 0.0020\n",
      "Epoch [16/50], Step [201/358], Loss: 0.0022\n",
      "Epoch [16/50], Step [211/358], Loss: 0.0015\n",
      "Epoch [16/50], Step [221/358], Loss: 0.0003\n",
      "Epoch [16/50], Step [231/358], Loss: 0.0014\n",
      "Epoch [16/50], Step [241/358], Loss: 0.0035\n",
      "Epoch [16/50], Step [251/358], Loss: 0.0016\n",
      "Epoch [16/50], Step [261/358], Loss: 0.0010\n",
      "Epoch [16/50], Step [271/358], Loss: 0.0006\n",
      "Epoch [16/50], Step [281/358], Loss: 0.0007\n",
      "Epoch [16/50], Step [291/358], Loss: 0.0014\n",
      "Epoch [16/50], Step [301/358], Loss: 0.0021\n",
      "Epoch [16/50], Step [311/358], Loss: 0.0004\n",
      "Epoch [16/50], Step [321/358], Loss: 0.0007\n",
      "Epoch [16/50], Step [331/358], Loss: 0.0008\n",
      "Epoch [16/50], Step [341/358], Loss: 0.0026\n",
      "Epoch [16/50], Step [351/358], Loss: 0.0029\n",
      "Epoch [17/50], Step [1/358], Loss: 0.0018\n",
      "Epoch [17/50], Step [11/358], Loss: 0.0040\n",
      "Epoch [17/50], Step [21/358], Loss: 0.0092\n",
      "Epoch [17/50], Step [31/358], Loss: 0.0013\n",
      "Epoch [17/50], Step [41/358], Loss: 0.0016\n",
      "Epoch [17/50], Step [51/358], Loss: 0.0008\n",
      "Epoch [17/50], Step [61/358], Loss: 0.0084\n",
      "Epoch [17/50], Step [71/358], Loss: 0.0067\n",
      "Epoch [17/50], Step [81/358], Loss: 0.0018\n",
      "Epoch [17/50], Step [91/358], Loss: 0.0014\n",
      "Epoch [17/50], Step [101/358], Loss: 0.0017\n",
      "Epoch [17/50], Step [111/358], Loss: 0.0010\n",
      "Epoch [17/50], Step [121/358], Loss: 0.0011\n",
      "Epoch [17/50], Step [131/358], Loss: 0.0014\n",
      "Epoch [17/50], Step [141/358], Loss: 0.0012\n",
      "Epoch [17/50], Step [151/358], Loss: 0.0011\n",
      "Epoch [17/50], Step [161/358], Loss: 0.0019\n",
      "Epoch [17/50], Step [171/358], Loss: 0.0006\n",
      "Epoch [17/50], Step [181/358], Loss: 0.0028\n",
      "Epoch [17/50], Step [191/358], Loss: 0.0020\n",
      "Epoch [17/50], Step [201/358], Loss: 0.0009\n",
      "Epoch [17/50], Step [211/358], Loss: 0.0006\n",
      "Epoch [17/50], Step [221/358], Loss: 0.0018\n",
      "Epoch [17/50], Step [231/358], Loss: 0.0011\n",
      "Epoch [17/50], Step [241/358], Loss: 0.0008\n",
      "Epoch [17/50], Step [251/358], Loss: 0.0034\n",
      "Epoch [17/50], Step [261/358], Loss: 0.0009\n",
      "Epoch [17/50], Step [271/358], Loss: 0.0005\n",
      "Epoch [17/50], Step [281/358], Loss: 0.0008\n",
      "Epoch [17/50], Step [291/358], Loss: 0.0010\n",
      "Epoch [17/50], Step [301/358], Loss: 0.0011\n",
      "Epoch [17/50], Step [311/358], Loss: 0.0009\n",
      "Epoch [17/50], Step [321/358], Loss: 0.0013\n",
      "Epoch [17/50], Step [331/358], Loss: 0.0012\n",
      "Epoch [17/50], Step [341/358], Loss: 0.0010\n",
      "Epoch [17/50], Step [351/358], Loss: 0.0024\n",
      "Epoch [18/50], Step [1/358], Loss: 0.0009\n",
      "Epoch [18/50], Step [11/358], Loss: 0.0006\n",
      "Epoch [18/50], Step [21/358], Loss: 0.0012\n",
      "Epoch [18/50], Step [31/358], Loss: 0.0005\n",
      "Epoch [18/50], Step [41/358], Loss: 0.0012\n",
      "Epoch [18/50], Step [51/358], Loss: 0.0014\n",
      "Epoch [18/50], Step [61/358], Loss: 0.0012\n",
      "Epoch [18/50], Step [71/358], Loss: 0.0008\n",
      "Epoch [18/50], Step [81/358], Loss: 0.0007\n",
      "Epoch [18/50], Step [91/358], Loss: 0.0024\n",
      "Epoch [18/50], Step [101/358], Loss: 0.0038\n",
      "Epoch [18/50], Step [111/358], Loss: 0.0013\n",
      "Epoch [18/50], Step [121/358], Loss: 0.0018\n",
      "Epoch [18/50], Step [131/358], Loss: 0.0013\n",
      "Epoch [18/50], Step [141/358], Loss: 0.0005\n",
      "Epoch [18/50], Step [151/358], Loss: 0.0011\n",
      "Epoch [18/50], Step [161/358], Loss: 0.0048\n",
      "Epoch [18/50], Step [171/358], Loss: 0.0019\n",
      "Epoch [18/50], Step [181/358], Loss: 0.0004\n",
      "Epoch [18/50], Step [191/358], Loss: 0.0014\n",
      "Epoch [18/50], Step [201/358], Loss: 0.0013\n",
      "Epoch [18/50], Step [211/358], Loss: 0.0013\n",
      "Epoch [18/50], Step [221/358], Loss: 0.0014\n",
      "Epoch [18/50], Step [231/358], Loss: 0.0019\n",
      "Epoch [18/50], Step [241/358], Loss: 0.0018\n",
      "Epoch [18/50], Step [251/358], Loss: 0.0010\n",
      "Epoch [18/50], Step [261/358], Loss: 0.0014\n",
      "Epoch [18/50], Step [271/358], Loss: 0.0010\n",
      "Epoch [18/50], Step [281/358], Loss: 0.0063\n",
      "Epoch [18/50], Step [291/358], Loss: 0.0031\n",
      "Epoch [18/50], Step [301/358], Loss: 0.0028\n",
      "Epoch [18/50], Step [311/358], Loss: 0.0026\n",
      "Epoch [18/50], Step [321/358], Loss: 0.0009\n",
      "Epoch [18/50], Step [331/358], Loss: 0.0019\n",
      "Epoch [18/50], Step [341/358], Loss: 0.0026\n",
      "Epoch [18/50], Step [351/358], Loss: 0.0013\n",
      "Epoch [19/50], Step [1/358], Loss: 0.0017\n",
      "Epoch [19/50], Step [11/358], Loss: 0.0015\n",
      "Epoch [19/50], Step [21/358], Loss: 0.0012\n",
      "Epoch [19/50], Step [31/358], Loss: 0.0011\n",
      "Epoch [19/50], Step [41/358], Loss: 0.0037\n",
      "Epoch [19/50], Step [51/358], Loss: 0.0010\n",
      "Epoch [19/50], Step [61/358], Loss: 0.0015\n",
      "Epoch [19/50], Step [71/358], Loss: 0.0020\n",
      "Epoch [19/50], Step [81/358], Loss: 0.0022\n",
      "Epoch [19/50], Step [91/358], Loss: 0.0005\n",
      "Epoch [19/50], Step [101/358], Loss: 0.0006\n",
      "Epoch [19/50], Step [111/358], Loss: 0.0019\n",
      "Epoch [19/50], Step [121/358], Loss: 0.0013\n",
      "Epoch [19/50], Step [131/358], Loss: 0.0006\n",
      "Epoch [19/50], Step [141/358], Loss: 0.0018\n",
      "Epoch [19/50], Step [151/358], Loss: 0.0013\n",
      "Epoch [19/50], Step [161/358], Loss: 0.0080\n",
      "Epoch [19/50], Step [171/358], Loss: 0.0009\n",
      "Epoch [19/50], Step [181/358], Loss: 0.0010\n",
      "Epoch [19/50], Step [191/358], Loss: 0.0011\n",
      "Epoch [19/50], Step [201/358], Loss: 0.0004\n",
      "Epoch [19/50], Step [211/358], Loss: 0.0008\n",
      "Epoch [19/50], Step [221/358], Loss: 0.0007\n",
      "Epoch [19/50], Step [231/358], Loss: 0.0014\n",
      "Epoch [19/50], Step [241/358], Loss: 0.0008\n",
      "Epoch [19/50], Step [251/358], Loss: 0.0005\n",
      "Epoch [19/50], Step [261/358], Loss: 0.0005\n",
      "Epoch [19/50], Step [271/358], Loss: 0.0032\n",
      "Epoch [19/50], Step [281/358], Loss: 0.0017\n",
      "Epoch [19/50], Step [291/358], Loss: 0.0006\n",
      "Epoch [19/50], Step [301/358], Loss: 0.0003\n",
      "Epoch [19/50], Step [311/358], Loss: 0.0004\n",
      "Epoch [19/50], Step [321/358], Loss: 0.0028\n",
      "Epoch [19/50], Step [331/358], Loss: 0.0016\n",
      "Epoch [19/50], Step [341/358], Loss: 0.0007\n",
      "Epoch [19/50], Step [351/358], Loss: 0.0020\n",
      "Epoch [20/50], Step [1/358], Loss: 0.0012\n",
      "Epoch [20/50], Step [11/358], Loss: 0.0028\n",
      "Epoch [20/50], Step [21/358], Loss: 0.0039\n",
      "Epoch [20/50], Step [31/358], Loss: 0.0013\n",
      "Epoch [20/50], Step [41/358], Loss: 0.0029\n",
      "Epoch [20/50], Step [51/358], Loss: 0.0010\n",
      "Epoch [20/50], Step [61/358], Loss: 0.0052\n",
      "Epoch [20/50], Step [71/358], Loss: 0.0005\n",
      "Epoch [20/50], Step [81/358], Loss: 0.0023\n",
      "Epoch [20/50], Step [91/358], Loss: 0.0010\n",
      "Epoch [20/50], Step [101/358], Loss: 0.0005\n",
      "Epoch [20/50], Step [111/358], Loss: 0.0024\n",
      "Epoch [20/50], Step [121/358], Loss: 0.0012\n",
      "Epoch [20/50], Step [131/358], Loss: 0.0010\n",
      "Epoch [20/50], Step [141/358], Loss: 0.0014\n",
      "Epoch [20/50], Step [151/358], Loss: 0.0006\n",
      "Epoch [20/50], Step [161/358], Loss: 0.0004\n",
      "Epoch [20/50], Step [171/358], Loss: 0.0014\n",
      "Epoch [20/50], Step [181/358], Loss: 0.0019\n",
      "Epoch [20/50], Step [191/358], Loss: 0.0009\n",
      "Epoch [20/50], Step [201/358], Loss: 0.0009\n",
      "Epoch [20/50], Step [211/358], Loss: 0.0006\n",
      "Epoch [20/50], Step [221/358], Loss: 0.0008\n",
      "Epoch [20/50], Step [231/358], Loss: 0.0009\n",
      "Epoch [20/50], Step [241/358], Loss: 0.0038\n",
      "Epoch [20/50], Step [251/358], Loss: 0.0004\n",
      "Epoch [20/50], Step [261/358], Loss: 0.0041\n",
      "Epoch [20/50], Step [271/358], Loss: 0.0024\n",
      "Epoch [20/50], Step [281/358], Loss: 0.0030\n",
      "Epoch [20/50], Step [291/358], Loss: 0.0007\n",
      "Epoch [20/50], Step [301/358], Loss: 0.0018\n",
      "Epoch [20/50], Step [311/358], Loss: 0.0028\n",
      "Epoch [20/50], Step [321/358], Loss: 0.0020\n",
      "Epoch [20/50], Step [331/358], Loss: 0.0025\n",
      "Epoch [20/50], Step [341/358], Loss: 0.0027\n",
      "Epoch [20/50], Step [351/358], Loss: 0.0014\n",
      "Epoch [21/50], Step [1/358], Loss: 0.0008\n",
      "Epoch [21/50], Step [11/358], Loss: 0.0022\n",
      "Epoch [21/50], Step [21/358], Loss: 0.0007\n",
      "Epoch [21/50], Step [31/358], Loss: 0.0015\n",
      "Epoch [21/50], Step [41/358], Loss: 0.0020\n",
      "Epoch [21/50], Step [51/358], Loss: 0.0008\n",
      "Epoch [21/50], Step [61/358], Loss: 0.0012\n",
      "Epoch [21/50], Step [71/358], Loss: 0.0005\n",
      "Epoch [21/50], Step [81/358], Loss: 0.0011\n",
      "Epoch [21/50], Step [91/358], Loss: 0.0005\n",
      "Epoch [21/50], Step [101/358], Loss: 0.0005\n",
      "Epoch [21/50], Step [111/358], Loss: 0.0012\n",
      "Epoch [21/50], Step [121/358], Loss: 0.0004\n",
      "Epoch [21/50], Step [131/358], Loss: 0.0011\n",
      "Epoch [21/50], Step [141/358], Loss: 0.0024\n",
      "Epoch [21/50], Step [151/358], Loss: 0.0008\n",
      "Epoch [21/50], Step [161/358], Loss: 0.0013\n",
      "Epoch [21/50], Step [171/358], Loss: 0.0019\n",
      "Epoch [21/50], Step [181/358], Loss: 0.0017\n",
      "Epoch [21/50], Step [191/358], Loss: 0.0009\n",
      "Epoch [21/50], Step [201/358], Loss: 0.0017\n",
      "Epoch [21/50], Step [211/358], Loss: 0.0010\n",
      "Epoch [21/50], Step [221/358], Loss: 0.0022\n",
      "Epoch [21/50], Step [231/358], Loss: 0.0010\n",
      "Epoch [21/50], Step [241/358], Loss: 0.0032\n",
      "Epoch [21/50], Step [251/358], Loss: 0.0016\n",
      "Epoch [21/50], Step [261/358], Loss: 0.0013\n",
      "Epoch [21/50], Step [271/358], Loss: 0.0063\n",
      "Epoch [21/50], Step [281/358], Loss: 0.0013\n",
      "Epoch [21/50], Step [291/358], Loss: 0.0021\n",
      "Epoch [21/50], Step [301/358], Loss: 0.0002\n",
      "Epoch [21/50], Step [311/358], Loss: 0.0017\n",
      "Epoch [21/50], Step [321/358], Loss: 0.0028\n",
      "Epoch [21/50], Step [331/358], Loss: 0.0013\n",
      "Epoch [21/50], Step [341/358], Loss: 0.0014\n",
      "Epoch [21/50], Step [351/358], Loss: 0.0007\n",
      "Epoch [22/50], Step [1/358], Loss: 0.0017\n",
      "Epoch [22/50], Step [11/358], Loss: 0.0010\n",
      "Epoch [22/50], Step [21/358], Loss: 0.0007\n",
      "Epoch [22/50], Step [31/358], Loss: 0.0011\n",
      "Epoch [22/50], Step [41/358], Loss: 0.0010\n",
      "Epoch [22/50], Step [51/358], Loss: 0.0008\n",
      "Epoch [22/50], Step [61/358], Loss: 0.0044\n",
      "Epoch [22/50], Step [71/358], Loss: 0.0014\n",
      "Epoch [22/50], Step [81/358], Loss: 0.0031\n",
      "Epoch [22/50], Step [91/358], Loss: 0.0016\n",
      "Epoch [22/50], Step [101/358], Loss: 0.0007\n",
      "Epoch [22/50], Step [111/358], Loss: 0.0035\n",
      "Epoch [22/50], Step [121/358], Loss: 0.0016\n",
      "Epoch [22/50], Step [131/358], Loss: 0.0015\n",
      "Epoch [22/50], Step [141/358], Loss: 0.0008\n",
      "Epoch [22/50], Step [151/358], Loss: 0.0023\n",
      "Epoch [22/50], Step [161/358], Loss: 0.0015\n",
      "Epoch [22/50], Step [171/358], Loss: 0.0026\n",
      "Epoch [22/50], Step [181/358], Loss: 0.0012\n",
      "Epoch [22/50], Step [191/358], Loss: 0.0013\n",
      "Epoch [22/50], Step [201/358], Loss: 0.0036\n",
      "Epoch [22/50], Step [211/358], Loss: 0.0010\n",
      "Epoch [22/50], Step [221/358], Loss: 0.0012\n",
      "Epoch [22/50], Step [231/358], Loss: 0.0018\n",
      "Epoch [22/50], Step [241/358], Loss: 0.0011\n",
      "Epoch [22/50], Step [251/358], Loss: 0.0009\n",
      "Epoch [22/50], Step [261/358], Loss: 0.0005\n",
      "Epoch [22/50], Step [271/358], Loss: 0.0007\n",
      "Epoch [22/50], Step [281/358], Loss: 0.0008\n",
      "Epoch [22/50], Step [291/358], Loss: 0.0008\n",
      "Epoch [22/50], Step [301/358], Loss: 0.0012\n",
      "Epoch [22/50], Step [311/358], Loss: 0.0007\n",
      "Epoch [22/50], Step [321/358], Loss: 0.0006\n",
      "Epoch [22/50], Step [331/358], Loss: 0.0012\n",
      "Epoch [22/50], Step [341/358], Loss: 0.0010\n",
      "Epoch [22/50], Step [351/358], Loss: 0.0009\n",
      "Early stopping at epoch 22\n"
     ]
    }
   ],
   "source": [
    "# 初始化早停参数\n",
    "early_stop_counter = 0\n",
    "early_stop_threshold = 5\n",
    "min_val_loss = np.inf\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:  # 假设你有一个验证集加载器val_loader\n",
    "            output = model(data)\n",
    "            val_loss = criterion(output, target)\n",
    "            val_losses.append(val_loss.item())\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    # 检查早停条件\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        early_stop_counter = 0  # 重置计数器\n",
    "        # 可以选择在这里保存模型的权重\n",
    "        torch.save(model.state_dict(), '../weights_cnn_lstm/cnn_lstm_{i}.pth'.format(i=i))\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    if early_stop_counter >= early_stop_threshold:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2e794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "state_dict = torch.load('../weights_cnn_lstm/cnn_lstm_{i}.pth'.format(i=i))\n",
    "model = CNNLSTMModel(input_channels = input_channels,middle_channels1 = middle_channels1, middle_channels2 = middle_channels2, \n",
    "                     out_channels = out_channels, hidden_size1 = hidden_size1, hidden_size2 = hidden_size2,hidden_size3 = hidden_size3).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        predictions.extend(output.cpu().numpy())\n",
    "\n",
    "# 反归一化预测结果\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c20d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb9392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 25.1473, MAE: 2.8927, RMSE: 5.0147\n"
     ]
    }
   ],
   "source": [
    "# 评估模型性能 32.1635\n",
    "mse = mean_squared_error(y_test_actual, predictions)\n",
    "mae = mean_absolute_error(y_test_actual, predictions)\n",
    "rmse = rmse(y_test_actual, predictions)\n",
    "print(f'MSE: {mse:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
